# 3D human pose estimation in video with temporal convolutions and semi-supervised training



## Abstract

​	在这篇paper中， 我们展示了视频中的3D姿态可以通过基于 **在2D关键点上、空洞时域卷积（dilated temporal convolutions)** 的全卷积模型来进行有效的预测 我们引进一种反向投影的简单、有效的半监督训练方法。这种方法使用的数据是未标注的视频数据。

​	首先我们使用已经预测好的2D关键点视频（未标注）作为输入，然后预测3D姿态， 最后反向投影回输入的2D关键点。



## 01 Introduce

​	我们的工作集中在视频中的3D人体姿态预测，我们使用了一种最新的方法，它首先进行2D关键点检测，然后进行3D关键点预测。尽管这种方法可以减少任务的难度，但是因为多个3D关键点可以映射到同一个2D关键点，因此它本质上是带有二义性的。

​	之前的工作，处理这种歧义性是通过RNN建立带有时域信息的模型。另一方面，卷积网络在处理带有时域信息的任务也很成功（传统是用RNN），例如神经机器翻译、语言建模、语音生成、语音识别。对比与RNN，CNN有一大优势，它可以并行的处理多个框架。

![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-29_15-19-05.png)

​	在这篇paper中，我们展示了一种全卷集架构，它在2D关键点上实施时域卷积操作，在视频中得到精确的3D姿态预测。我们的方法可以兼容任何的2D关键点检测器，并且通过扩充的卷积维度，可以有效的处理大的上下文信息。比较于基于RNN的方法，它准确率更高、简洁、有效（无论是从计算复杂度还是从参数数目的角度）。

​	有了高的准确率和有效的架构，我们转向去处理一些只有很少带标签的训练数据集，然后引进一种新的策略去使用未标签的视频数据集进行半监督学习。带标签的资源不足时3D视频姿态预测的最大挑战，因为一般来说这种模型需要大量的标签数据。我们的方法受到了非监督机器翻译的启发（句子从一种语言被翻译到另一种语言， 然后在翻译回原语言），具体来说，我们使用无标签的2D视频，通过现有的姿态检测器预测出3D姿态，然后再映射回2D空间。

​	总结，这篇paper有两大贡献：第一，对于3D姿态预测，我们展示了一种基于2D关键点轨迹之上扩张出一种时域卷积方法，它兼具简洁和高效的特性。在相同精确度的基准之上，我们的模型不管在计算复杂度上还是在模型参数上都要比基于RNN的模型更加简洁。

​	第二，我们引进一种半监督的方法来处理未标记的视频，在缺少标签数据的情况下也可以取得好的效果。比较于之前的半监督方法，只需要照相机原来的一些参数，不需要实际的2D标签数据或者照相机之外的其他参数。



## 02 Related Work

​	在深度学习取得成功之前，大多数进行3D姿态预测的方法都是基于特征工程和假设骨架和关节的流动性来完成的。第一种基于卷积神经网络的方法聚焦于端到端的重建，无需中间监督，直接从RGB图片来预测3D姿态。

​	另一种是**Two-step pose estimation**， 这是一种新的3D预测家族，他是基于性能最好的2D关键点检测器，第一步在图片空间中预测出关节点的位置，接下来在转移到3D空间。这种模式的性能要好于第一种，因为它利用了中间监督的特性。早起的方法从一个大的2D关键点集合中通过k最邻近算法来搜索出预测的关键点集合，然后简单的输出对应的3D关键点。一些方法图片特性和实际的姿态。另外，可以通过对一组2D关键点对它们的深度预测来生成3D姿态。

​	**Video pose estimation**. 之前的大多数预测都是在单帧的环境中， 但最近的一些工作，通过处理视频，通过利用其时域信息，产生出更健壮的预测、对噪声也有了更好的抗性。比如：从时空体积块（spatial-temporal volumes) 的梯度直方图（HoG histograms of oriented gradients）中推断出3D姿态。通过Bi-directional LSTMs 精调(refine)从单张图片预测出的3D姿态。然而最成功的方法是学习2D关键点的轨迹，我们便是基于这种方法。

​	最近，LSTM序列到序列的学习模型很受欢迎，它将是视频中的一个2D姿态序列编码到一个固定的向量中去，然后再对这个向量解码到3D姿态中。然而输入和输出的序列有相同的长度，一个2D姿态确定性传输是一个更自然的选择。我们的带有seq2seq模型表明输出的姿态趋向于很长的序列。处理这个问题可以以时间连续性为代价，每5帧重新初始化一下这个编码器。考虑到之前身体部位的连续性，RNN方法也可以得到相同的效果。

​	**半监督训练**。这些可以用于多任务网络：2D、3D姿态预测和动作识别， 使用2D/3D标签的Human3.6M数据集和仅仅有2D标签的MPII数据集。未标签的多角度视频数据集也被用于3D姿态预训练表示。在第二个数据集中（仅仅有2D姿态标记），GAN可以不真实的2D姿态中辨别实际的2D姿态。`Ordinal depth
supervision for 3d human pose estimation`提出了一种弱监督学习的方法，它基于序列的深度标记，通过深度对比增强数据集，比如：这个右腿在左腿之后。我们的工作不同于以上，我们并没有使用heatmap,而是通过检测关键点的坐标来描述姿态，heatmap传递更多的信息，但是需要计算量更大的2D卷积（3D卷集如果时间维提供），这些精确度取决于heatmap的分辨率。我们的dense model只需很少的参数就可以达到很高的精确度，在训练和实现速度上效果更好；通过执行一维的卷积操作来使用时间信息； 提出了几种优化操作可以达到更低的3D重建错误；没有使用seq2seq 模型，我们学习一种确定性映射。比较于之前所说的两步模型（使用的是栈沙漏网络用于2D关键点检测），我们使用的是MASK RCNN和CPN（cascaded pyramid network) 检测器，这对于3D姿态预测来说，更加健壮。



## 03 时域扩张卷积模型 

​	模型是由残差连接构成的全卷集网络，把一系列，密集的2D姿态作为输入，并**将时间卷积和线性投影层交替**。卷集模型能够参数化批数据和时间维度，这是RNN模型做不到的，它不能在时间维度之上进行参数化。（？）。在卷集模型中，不管序列的长度是多少，从输入到输入的梯度路径有一个固定的长度，这可以减轻受RNN影响的梯度消失和梯度爆炸。卷集架构也可以对时间感受野给予精确的控制，这有益于建模3D姿态估计任务的时间依赖性。同时我们部署一个扩张的卷集层，在保持时间有效性的同时，对长期的依赖性进行建模。这种带有额外卷集层的架构已经在语音产生、语义分割、机器翻译上都非常有效。	

​	这种架构**首先**把每一帧的 J（关节点）的x,y 坐标作为输入，同时应用一个核大小为W的卷积层，然后输出C个特征。**接下来**是B个ResNet风格的blocks，它们被一个skip-connection所包围。每一个block首先执行带有filter大小为W，扩张因子（dilation factor)D(=$W^B$)的一维卷积层，**然后**是一个线性投影（W=1，D=1的卷集），**再然后**是batch normalization,ReLU和dropout. 每一个block都通过W指数增长其感受野，同时参数个数只是线性增加。这个filter超参数W和D的设置是为了让任何输入帧的感受野能够形成树结构并覆盖所有输入帧（见图一）。**最后**，这最后一层输出对应一个序列的所有输入帧（使用过去和未来的数据）的3D姿态预测。为了评估我们方法的实时场景，我们也实验了因果卷积（casual convolution, 仅仅接受之前的数据帧）。我们在附录A.1中展示了因果卷积和空洞卷集（dilated convolution）的比较。

​	卷积通常为零填充，以获得与输入一样多的输出。然而我们在早些时候的实验中发现：这导致了边界效应并增加了损失。相反，我们将输入序列与边界帧的复制填充到序列的左侧和右侧（参见附录A.5，图9a中的插图）并执行有效（即未填充）卷积。

![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_14-23-21.png)

​	图2显示了我们的体系结构的实例化，其中感知字段(receptive filed)大小为243帧，B = 4个块。 对于卷积层，我们设置W = 3，C = 1024输出特征，我们使用丢失率(dropout rate) p = 0.25。



## 04 半监督方法

​	因为获得实际的3D姿态标注非常困难，我们引入了一种半监督训练方法，以提高标记3D地面实况姿势数据受限的设置的准确性。我们利用未标记的视频与现成的2D关键点检测器相结合，通过反向投影损耗项来扩展监督损失函数。关键思想是利用未标记数据解决自动编码问题，其中3D姿态估计器用作编码器，然后将预测姿势映射回2D空间，基于此可以计算重建损失。

​	![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_14-32-54.png)

​	上图是我们方法的一个概括。特别的，包含一个监督的部件和一个充当正则器的非监督部件。这两个对象（包含一般批数据的标签数据，和另一半的非标签数据）。对于标签数据，我们使用实际的3D姿态作为目标来训练监督学习，未标记的数据用于实现自动编码器丢失，其中预测的3D姿势被投射回2D，然后检查与输入的一致性。

​	**轨迹模型**，由于透视投影，屏幕上的2D姿势取决于轨迹（即，在每个时间步长处空间中人参考坐标的位置）和3D姿势（人参考坐标中的关节的位置）。因此，我们还对人的3D轨迹进行回归，以便可以正确地执行到2D反向投影。为此，我们优化了第二个网络，该网络使相机空间的全局轨迹回归。在将其投影回2D之前，将后者添加到姿势中。这两个网络具有相同的架构但不共享任何权重，因为我们观察到它们在以多任务方式训练时会产生负面影响。我们对人的3D轨迹进行回归，并添加软约束以匹配未标记预测的平均骨长度与标记的预测。所有的都是联合训练，WMPJPE 代表 “weighted MPJPE（Mean per-joint position error)”。

在以多任务方式训练时，其他负面影响。 由于如果拍摄对象远离相机变得越来越难以回归精确轨迹，我们优化轨迹的加权平均每关节位置误差（WMPJPE）损失函数：

​	$$E = \frac{1}{y_z} || f(x) - y||$$ 

我们对每个样本对它实际的深度（$y_z$离相机的距离）的倒数作为权重参数。回归一个远距离的对象轨迹也不是我们的实验目的，因为在一个小的区域，相对的2D关键点去相遇集中在一起。

​	**骨长L2损失。** 我们希望激励合理的3D姿势的预测，而不是仅仅复制输入。 为此，我们发现添加软约束以使未标记批次中的平均骨长度与标记批次的大致**匹配**（图3中的“骨长度L2损失”）是有效的。 正如我们在§6.2中所示，这个在自我监督中起着重要作用。

​	**讨论**。我们的方法仅仅需要照相机的内在参数（焦距，主点 principle point 和可选的偏斜 optionally skew）。该方法不依赖于任何特定的网络架构，并且可以应用于将2D关键点作为输入的任何3D姿态检测器。 在我们的实验中，我们使用§3中描述的架构将2D姿势映射到3D。为了将3D姿势投影到2D，我们使用一个简单的投影层来考虑线性参数（焦距，主点）以及非线性失真系数（切向和径向）。 我们观察到非线性项对最终结果的影响可以忽略不计，从而增加了我们对广泛输入视频的适用性。

### 05 实验装置

#### 1 - 数据库和测评

​	我们评估了两个动作捕捉数据集，Human3.6M [18,17]和HumanEva-I [39]。 Human3.6M包含11个主题的360万个视频帧，其中7个用3D姿势注释。每个主体使用四个50 Hz的同步摄像机执行15个动作，这些动作记录在视频中。在之前的工作[34,44,30,42,8,33,47,29]之后，我们采用了一个17关节的骨架，对五个主题（S1，S5，S6，S7，S8）进行训练，并进行了测试两个测试对象（S9和S11）。我们为所有行动训练单一模型。
​	HumanEva-I是一个小得多的数据集，包含从三个摄像机视图以60 Hz记录的三个主题。在[30,14]之后，我们通过训练每个动作的不同模型（单一动作 -  SA）来评估三个动作（Walk，Jog，Box）。我们还在训练所有动作的一个模型（多动作 -  MA）时报告结果，如[34,24]。我们采用15关节骨架并使用提供的训练/测试分割。
​	在我们的实验中，我们考虑了三个评估协议：**方案1**是以毫米为单位的平均每关节位置误差（MPJPE），它是预测关节位置和实际关节位置之间的平均欧几里德距离，并遵循[26, 45,50,30,34]。**方案2**报告了与实际标记（P-MPJPE）刚性对齐后的误差[30,42,8,33,47,14]。对于半监督实验，方案3将预测的姿势与仅[37]之后的比例（N-MPJPE）的实际标签对齐。

#### 2 - 2D姿态预测的实施详情

​	大多数先前的工作[30,49,44]从实际边界框中提取主体，然后应用堆叠的沙漏探测器来预测实际边界框内的2D关键点位置[32]。 我们的方法（§3和§4）不依赖于任何特定的2D关键点检测器。 因此，我们研究了几个不依赖于实际框的2D探测器。 除了堆叠沙漏探测器之外，我们还研究了Mask R-CNN [10]和ResNet-101-FPN [27]骨干，利用其在Detectron中的参考实现，以及级联金字塔网络（CPN）[ 4]代表FPN的扩展。 CPN实现要求外部提供边界框（我们在这种情况下使用Mask R-CNN框）。ablation study(为了研究模型中所提出的一些结构是否有效而进行的实验)中，我们还尝试将我们的3D姿势估计器直接应用于预训练的2D COCO关键点，以估计Human3.6M的3D关节。

​	对于Mask R-CNN，我们采用ResNet-101骨干，训练“stretched1x”策略[10] . 当在Human3.6M上微调模型时，我们重新初始化关键点网络的最后一层，以及 deconv层回归热图以学习一组新的关键点。 我们在4个GPU上进行训练，逐步衰减学习速率：1e-3用于60k迭代，然后1e-4用于10k迭代，1e-5用于10k迭代。 在推论中，我们在热图上应用softmax并提取所得2D分布的预期值（soft-argmax）。 这导致比hard-argmax更平滑和更精确的预测[29]。

> 1x 方案：初始LR=0.002, 并在60k 和 80k 之后衰减 *0.1, 在90次迭代终止
>
> stretched 1x, 该方案将1x方案，拉伸了1.44x， 并延长了第一个学习率的持续时间。在100k和120k次迭代后衰减*0.1

​	对于CPN，我们使用ResNet-50主干，分辨率为384×288。 为了微调，我们重新初始化GlobalNet和RefineNet的最终层（卷积权重和批量规范化统计）。 接下来，我们在一个GPU上训练，批量为32个图像并逐步衰减学习率：5e-5（初始值的1/10）用于6k迭代，然后5e-6用于4k迭代，最后 5e-7用于2k次迭代。 我们在微调时保持批量标准化。 我们使用实际边界框进行训练，并使用微调Mask R-CNN模型预测的边界框进行测试。

#### 3 - 3D 姿态预测的实现细节

​	为了与其他工作[30,26,45,50,30,34]保持一致，我们通过根据逆相机变换旋转和平移实际姿势来训练和评估相机空间中的3D姿势，而不是使用全局轨迹（半监督设置除外，§4）。

​	我们使用Amsgrad [35]作为优化器，并训练80个epoch。对于Human3.6M，我们采用指数降低学习率计划，从η= 0.001开始，每个时期应用收缩因子α= 0.95。
​	所有时间模型，即具有大于1的感受野的模型，对姿势序列中的样本的相关性(correlation)比较敏感（参见§3）。这导致批量标准化的偏差统计量，其假设为独立样本[15]。在初步实验中，我们发现在训练期间，比较于**不利用时间信息**（在批次中具有良好随机化样本的模型），**预测大量相邻帧** 会产生更糟糕的结果。因此，我们通过选择来自不同视频片段的训练片段来减少训练样本中的相关性。剪辑集大小设置为我们体系结构的感知字段的宽度，以便模型预测每个训练剪辑的单个3D姿势。这对于概括很重要，我们在附录A.5中详细分析。	

​	我们可以通过用跨步卷积（strided convolution)代替空洞卷积来大大优化这种单帧设置，其中步幅设置为膨胀系数（参见附录A.6）。 这避免了计算从未使用的状态，我们仅在训练期间应用此优化。 在interface中，我们可以处理整个序列并重用其他3D帧的中间状态，以便更快地推断。 这是可能的，因为我们的模型在时间维度上不使用任何形式的池化层。 为了避免帧丢失到有效卷积，我们通过复制填充，但仅在序列的边界处（附录A.5，图9a显示了图示）。

​	我们观察到批量标准化的默认超参数导致测试误差（±1 mm）的大幅波动以及推断的运行估计值的波动。 为了获得更稳定的运行统计数据，我们使用批量标准化动量β的时间表：我们从β= 0.1开始，并以指数方式衰减，使其在最后时期达到β= 0.001。

## 06 结果

####  时域空洞卷积模型

![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_17-57-06.png)

​	表1显示了我们的卷积模型的结果，其中B = 4块，两个评估协议的感知域为243个输入帧（§5）。 该模型的平均误差低于两种协议下的所有其他方法，并且不依赖于其他数据，如许多其他方法（+）。 根据方案1（表1a），我们的模型在平均值上优于先前的最佳结果[24] 6毫米，相当于减少了11％的误差。 值得注意的是，[24]使用实际框，而我们的模型没有。该模型清楚地利用了时间信息，因为与单帧基线相比，协议1的误差平均高出约5 mm，我们将所有卷积滤波器的宽度设置为W = 1。 用于高度动态的动作，例如“Walk”（6.7 mm）和“Walk Together”（8.8 mm）。 具有因果卷积(casual conv model) 的模型的性能大约是单帧基线和我们模型之间的一半; 因果卷积通过预测最右边输入帧的3D姿势来实现在线处理。有趣的是，实际边界框的结果与使用Mask R-CNN的预测边界框具有相似的性能，这表明在我们的单人场景中预测几乎是完美的。 图4显示了预测的姿势示例，包括预测的2D关键点，我们在补充材料（附录A.7）以及https://dariopavllo.github.io/ VideoPose3D中包含视频插图。

​	接下来，我们评估2D关键点检测器对最终结果的影响。 表3报告了我们的模型对比其他模型的准确性，来自[30]的沙漏网络预测（均在MPII上预训练并在Human3.6M上进行了微调），Detectron和CPN（均经过预先训练 COCO并在Human3.6M上进行了微调。 Mask R-CNN和CPN都比堆叠沙漏网络提供更好的性能。 这种改进可能是由于更高的热图分辨率，更强的特征组合（功能金字塔网络[27,36]用于Mask R-CNN和RefineNet用于CPN），以及它们预先训练的数据集越多，即COCO [28。 当训练二维实际的姿态时，我们的模型将[30]的下限提高了8.3 mm，并且Lee等人采用基于LSTM的方法。 [24]协议1为1.2毫米。因此，我们的改进不仅仅是因为更好的2D探测器。

​	**绝对位置误差不能测量随时间推移的预测的平滑性**，这对于视频很重要。 为了评估这一点，我们测量联合速度误差（MPJVE），对应于3D姿势序列的一阶导数的MPJPE。 表2显示，我们的时间卷积模型将单帧基线的MPJVE平均降低了76％，从而使姿势更加平滑。

​	表4显示了HumanEva-I的结果，我们的模型推广到较小的数据集; 结果基于预先训练的Mask R-CNN 2D检测。 我们的单动作和多动作模型都以超大的优势超越了之前的先进技术。

![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_18-13-00.png)





|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_18-13-27.png) | ![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_18-13-39.png) |



​	最后，表5在复杂性方面将卷积模型与[14]的LSTM模型进行了比较。我们报告模型参数的数量和浮点运算（FLOP）的估计，以在推理时间预测一帧（详见附录A.2）。对于后者，我们只考虑矩阵乘法，并在无限长度的假设序列上报告摊销成本（忽略填充）。 MPJPE结果基于在没有测试时间增强的情况下在地面实况2D姿势上训练的模型。即使计算次数减半，我们的模型也能实现明显更低的误差。我们最大的243帧感受野模型与[14]具有大致相同的复杂性，但误差降低3.8 mm。该表还强调了空洞卷积的有效性，这种卷积在感受野方面仅以对数方式增加了复杂性。


​	由于我们的模型是卷积的，因此可以在序列数量和时间维度上并行化。这与RNN形成对比，**RNN只能在不同的序列上并行化**，因此对于小批量大小效率要低得多。对于推论，我们在单个NVIDIA GP100 GPU上测量了大约150k FPS的单个长序列，即批量大小为1，假设2D姿势已经可用。然而，由于并行处理，速度在很大程度上取决于批量大小。

### 半监督方法

​	我们采用[37]的设置，他们将Human3.6M训练集的各种子集视为标记数据，并将剩余样本用作未标记数据他们的设置通常还将所有数据下采样到10 FPS（从50 FPS）。 标记的子集是通过首先减少subject数量然后通过下采样subject 1来创建的。

​	由于数据集是下采样的，我们使用9帧的感知域，相当于45帧上采样。 对于非常小的子集，S1的1％和5％，我们使用3帧，并且我们使用单帧模型用于0.1％的S1，其中仅有49帧可用。 我们仅在标记数据上微调CPN，并通过仅对几个时期的标记数据进行迭代来加热训练（对于较小的子集，1个时期≥1S，20个时期）

|                                                              |                                                              |
| ------------------------------------------------------------ | :----------------------------------------------------------- |
| ![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_18-45-51.png) | 图5a显示，随着标记数据量的减少，我们的半监督<br/>方法变得更加有效。对于标记帧小于5K的设置，我们<br/>的方法在我们的监督基线上实现了约9-10.4 mm <br/>N-MPJPE的改进。我们的监督基线比[37]更强大，<br/>并且大大优于他们的所有结果。<br/> <br/> 图5b显示了针对数据集的非下采样版本（50 FPS）<br/>的更常见的协议1下的我们的方法的结果。这种设<br/>置更适合我们的方法，因为它允许我们利用视频中<br/>的完整时间信息。这里我们使用27帧的感知场，除了<br/>1％的S1，我们使用9帧，和0.1％的S1，我们使用一帧。<br/>我们的半监督方法在监督基线上获得高达14.7<br/> mm的MPJPE。<br/>
图5c切换CPN 2D关键点以获得地面实况2D姿势，<br/>以测量我们是否可以使用更好的2D关键点检测器<br/>更好地执行。在这种情况下，改进可以达到22.6 mm <br/>MPJPE（S1的1％），这可以确保更好的2D检测可以<br/>提高性能。同一图表显示骨骼长度项对于预测有<br/>效姿势至关重要，因为它迫使模型反映运动学约束<br/>（“我们的半监督GT abl。”这一行）。删除该<br/>术语会大大降低半监督训练的有效性：对于1％<br/>的S1，误差从78.1 mm增加到91.3 mm，而监督<br/>基线则为100.7 mm。 |



## 07 结论

​	我们为视频中的3D人体姿态估计引入了一个简单的完全卷积模型。 我们的架构利用2D关键点轨迹上的空洞卷积来利用时间信息。 这项工作的第二个贡献是反向投影，这是一种半监督的训练方法，用于在标记数据稀缺时提高性能。 该方法适用于未标记的视频，仅需要内部摄像机参数。
​	我们的完全卷积体系结构通过6mm平均关节误差改善了Human3.6M数据集的先前最佳结果，相当于相对减少了11％，并且还显示了HumanEva-I的大幅改进。 当有5K或更少的注释帧时，反向投影可以在强基线上提高约10mm N-MPJPE（15mm MPJPE）的3D姿态估计精度。



## References

![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_19-00-54.png) 

![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-12-31_19-01-15.png)

